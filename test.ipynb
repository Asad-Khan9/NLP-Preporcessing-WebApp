{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural language processing is a field of artificial intelligence that deals with the interaction between computers and human natural language\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "\n",
    "# input text\n",
    "text = \"Natural language %processing? is a field of? artificial intelligence, that deals with the interaction@ between computers and human (natural) language.\"\n",
    "\n",
    "tokens = nltk.word_tokenize(text)\n",
    "\n",
    "filtered_tokens = [token for token in tokens if token not in string.punctuation]\n",
    "\n",
    "\n",
    "print(\" \".join(filtered_tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         text_column\n",
      "0  Natural language processing is a field of arti...\n",
      "1                  Another sentence with punctuation\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "def removePunctuation(df, column):\n",
    "    # Tokenize the text in the specified column for each row in the DataFrame\n",
    "    df[column] = df[column].apply(lambda text: nltk.word_tokenize(text))\n",
    "\n",
    "    # Remove punctuation from tokens for each row\n",
    "    df[column] = df[column].apply(lambda tokens: [token for token in tokens if token not in string.punctuation])\n",
    "\n",
    "    # Join the tokens back into text for each row\n",
    "    df[column] = df[column].apply(lambda tokens: \" \".join(tokens))\n",
    "\n",
    "    return df\n",
    "\n",
    "# Example usage:\n",
    "# Create a sample DataFrame\n",
    "data = {'text_column': [\"Natural language %processing? is a field of? artificial intelligence, that deals with the interaction@ between computers and human (natural) language.\",\n",
    "                        \"Another sentence with punctuation!\"]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Call the function to remove punctuation from the 'text_column'\n",
    "df_cleaned = removePunctuation(df, 'text_column')\n",
    "\n",
    "print(df_cleaned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current data type of 'Borderlands' column: object\n",
      "Updated data type of 'Borderlands' column: object\n"
     ]
    }
   ],
   "source": [
    "data_path = \"dataset\\\\twitter_training.csv\"\n",
    "temp_df = pd.read_csv(data_path)\n",
    "\n",
    "# Check the current data type of the column\n",
    "print(\"Current data type of 'Borderlands' column:\", temp_df[\"Borderlands\"].dtype)\n",
    "\n",
    "# Convert the data type of the column to string\n",
    "temp_df[\"Borderlands\"] = temp_df[\"Borderlands\"].astype(str)\n",
    "\n",
    "# Check the updated data type of the column\n",
    "print(\"Updated data type of 'Borderlands' column:\", temp_df[\"Borderlands\"].dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['2401', 'Borderlands', 'Positive',\n",
       "       'im getting on borderlands and i will murder you all ,'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   2401  Borderlands  Positive  \\\n",
      "0  2401  Borderlands  Positive   \n",
      "1  2401  Borderlands  Positive   \n",
      "2  2401  Borderlands  Positive   \n",
      "3  2401  Borderlands  Positive   \n",
      "4  2401  Borderlands  Positive   \n",
      "\n",
      "                                                 new  \n",
      "0  I am coming to the borders and I will kill you...  \n",
      "1  im getting on borderlands and i will kill you all  \n",
      "2  im coming on borderlands and i will murder you...  \n",
      "3  im getting on borderlands 2 and i will murder ...  \n",
      "4  im getting into borderlands and i can murder y...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "data_path = \"dataset\\\\twitter_training.csv\"\n",
    "temp_df = pd.read_csv(data_path)\n",
    "# Rename the column to \"new\"\n",
    "temp_df.rename(columns={\"im getting on borderlands and i will murder you all ,\": \"new\"}, inplace=True)\n",
    "\n",
    "# Define a function to remove punctuation using NLTK\n",
    "def remove_punctuation(text):\n",
    "    if isinstance(text, str):\n",
    "        tokenizer = nltk.RegexpTokenizer(r'\\w+')\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "        return ' '.join(tokens)\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "# Apply the function to the \"new\" column\n",
    "temp_df[\"new\"] = temp_df[\"new\"].apply(remove_punctuation)\n",
    "\n",
    "# Print the first few rows of the DataFrame to verify the changes\n",
    "print(temp_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Python programmers often tend like program in'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "# nltk.download(\"wordnet\")\n",
    "# nltk.download(\"omw-1.4\")\n",
    "def lemmatize(text):\n",
    "    wnl = WordNetLemmatizer()\n",
    "    if isinstance(text, str):\n",
    "        text_tokens = nltk.word_tokenize(text)\n",
    "        lemmatized_tokens = [wnl.lemmatize(word, pos=\"v\") for word in text_tokens]\n",
    "        return \" \".join(lemmatized_tokens)\n",
    "\n",
    "    \n",
    "lemmatize(\"Python programmers often tend like programming in\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cudf'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcudf\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcudf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataFrame\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdask_cuda\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LocalCUDACluster\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'cudf'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import cudf\n",
    "from cudf import DataFrame\n",
    "from dask_cuda import LocalCUDACluster\n",
    "from dask.distributed import Client\n",
    "import cunlp\n",
    "\n",
    "# Initialize ROCm\n",
    "cunlp.init(backend=\"rocm\")\n",
    "\n",
    "# Create a Dask-CUDA cluster and client\n",
    "cluster = LocalCUDACluster()\n",
    "client = Client(cluster)\n",
    "\n",
    "def remove_stopwords(df, column):\n",
    "    # Convert pandas DataFrame to cuDF DataFrame\n",
    "    gdf = cudf.from_pandas(df)\n",
    "\n",
    "    # Remove stop words using cuNLP\n",
    "    stopwords = cunlp.stopwords(\"en\")\n",
    "    gdf[column] = gdf[column].str.remove_stopwords(stopwords=stopwords)\n",
    "\n",
    "    # Convert back to pandas DataFrame\n",
    "    processed_df = gdf.to_pandas()\n",
    "    return processed_df\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Path to the dataset\n",
    "    data_path = \"dataset\\\\WAZE_REVIEWS.csv\"\n",
    "\n",
    "    # Load your dataframe\n",
    "    df = pd.read_csv(data_path)\n",
    "\n",
    "    # Scatter the DataFrame across the Dask-CUDA cluster\n",
    "    ddf = client.scatter(df)\n",
    "\n",
    "    # Process the DataFrame in parallel on the GPU\n",
    "    processed_ddf = ddf.map_partitions(remove_stopwords, column=\"review_text\", meta=df)\n",
    "\n",
    "    # Gather the processed DataFrame back to the main process\n",
    "    processed_df = client.gather(processed_ddf)\n",
    "\n",
    "    print(processed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       TRENDING : New Yorkers encounter empty superma...\n",
      "1       could n't find hand sanitizer Fred Meyer , tur...\n",
      "2               Find protect loved ones # coronavirus . ?\n",
      "3       # Panic buying hits # NewYork City anxious sho...\n",
      "4       # toiletpaper # dunnypaper # coronavirus # cor...\n",
      "                              ...                        \n",
      "3793    Meanwhile Supermarket Israel -- People dance s...\n",
      "3794    panic buy lot non-perishable items ? ECHO need...\n",
      "3795    Asst Prof Economics @ cconces @ NBCPhiladelphi...\n",
      "3796    Gov need somethings instead biar je rakyat ass...\n",
      "3797    @ ForestandPaper members committed safety empl...\n",
      "Name: OriginalTweet, Length: 3798, dtype: object\n",
      "2.0049993991851807\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "data_path = \"dataset\\\\Corona_NLP_test.csv\"\n",
    "\n",
    "df1 = pd.read_csv(data_path)\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    if isinstance(text, str):\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "        filtered = [token for token in tokens if token.lower() not in stopwords]\n",
    "        return ' '.join(filtered)\n",
    "start = time.time()\n",
    "df1[\"OriginalTweet\"] = df1[\"OriginalTweet\"].apply(remove_stopwords)\n",
    "print(df1[\"OriginalTweet\"])\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'nvidia-smi' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
